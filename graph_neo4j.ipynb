{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain neo4j openai wikipedia tiktoken langchain-openai\n",
    "# from https://github.com/tomasonjo/blogs/blob/master/llm/openaifunction_constructing_graph.ipynb\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path='secrets.env')\n",
    "graph = Neo4jGraph(\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Property(BaseModel):\n",
    "    key: str = Field(..., description=\"key\")\n",
    "    value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(None, description=\"node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(None, description=\"relationship properties\")\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    nodes: List[Node] = Field(..., description=\"nodes in the graph\")\n",
    "    rels: List[Relationship] = Field(..., description=\"relationships in the graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return first_word + \"\".join(capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    properties = {}\n",
    "    if not props:\n",
    "        return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "        allowed_nodes: Optional[List[str]] = None,\n",
    "        allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\n",
    "            \"system\",\n",
    "            f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "    ## 1. Overview\n",
    "    You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "    - **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "    - The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "    ## 2. Labeling Nodes\n",
    "    - **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "    - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "    - **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "    {'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "    {'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "    ## 3. Handling Numerical Data and Dates\n",
    "    - Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "    - **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "    - **Property Format**: Properties must be in a key-value format.\n",
    "    - **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "    - **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "    ## 4. Coreference Resolution\n",
    "    - **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "    If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
    "    always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
    "    Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
    "    ## 5. Strict Compliance\n",
    "    Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "            \"\"\"),\n",
    "            (\"human\", \"Use the given format to extract info from the following input: {input}\"),\n",
    "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "        ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "      document:Document,\n",
    "      nodes:Optional[List[str]] = None,\n",
    "      rels:Optional[List[str]]=None) -> GraphDocument:\n",
    "    \n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document.page_content)['function']\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "        \n",
    "    graph.add_graph_documents([graph_document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Read the wikipedia article\n",
    "raw_documents = WikipediaLoader(query=\"History of France\").load()\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=600, chunk_overlap=24)\n",
    "\n",
    "# Only take the first the raw_documents\n",
    "documents = text_splitter.split_documents(raw_documents)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Event A came before Event B.\n",
    "Event B came before Event C.\n",
    "Event C came before Event A.\n",
    "\"\"\"\n",
    "documents = [Document(page_content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:24<00:00, 20.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    extract_and_store_graph(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the graph\n",
    "graph.query(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = \"Which countries has France had a conflict with?\"\n",
    "\n",
    "decide_continue_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a highly intelligent agent whose task is to alter original prompts to make similar prompts which are worded differently. Use synonyms, paraphrases, and other techniques to alter the original prompts.'},\n",
    "        {'role': 'user', 'content': \n",
    "            'Generate 9 prompts in a numbered list with no extra newlines. The original prompt is:\\n' \n",
    "            + original_query\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = decide_continue_response.choices[0].message.content\n",
    "# make a list of the 9 prompts\n",
    "prompts = response.split('\\n')\n",
    "for i in range(len(prompts)):\n",
    "    prompts[i] = prompts[i][3:]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=ChatOpenAI(temperature=0, model=\"gpt-4o\"),\n",
    "    qa_llm=ChatOpenAI(temperature=0, model=\"gpt-4o\"),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")\n",
    "outputs = []\n",
    "for prompt in prompts:\n",
    "    res = cypher_chain.run(prompt)\n",
    "    if res is not \"I don't know the answer.\":\n",
    "        outputs.append(res)\n",
    "\n",
    "# merge outputs into string\n",
    "context = \"\\n\".join(outputs)\n",
    "\n",
    "query_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a highly intelligent agent whose task is to answer questions only based on the context you are given. Your answers must be concise and very short.'},\n",
    "        {'role': 'user', 'content': \n",
    "            'The original question is: '+ original_query + '\\n'\n",
    "            'The given context is: ' + context + '\\n'\n",
    "            'Your task is to answer the original question using only the context you are given. Your answer must be concise and very short.'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(query_response.choices[0].message.content)\n",
    "\n",
    "\n",
    "# embed query\n",
    "# embed all edges \n",
    "# cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cypher\n",
    "MATCH (france:Country {name: \"France\"})-[:HASCONFLICT]->(war:War)<-[:PARTICIPATED_IN]-(country:Country)\n",
    "RETURN DISTINCT country.name\n",
    "\n",
    "cypher\n",
    "MATCH (france:Country {name: \"France\"})-[:HASCONFLICT]->(conflictCountry:Country)\n",
    "RETURN conflictCountry.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) \n",
    "def ask_gpt_if_same(node1: str, node2: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[    \n",
    "            # System basically tells the chat gpt model how to act\n",
    "            {\"role\": \"system\", \"content\": \"You are the most intelligent comparison agent in the world. You can compare any two pieces of text and accurately tell if they refer to the same thing or not.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Do the following two nodes mean exactly the same thing? Node 1: \" + node1 + \". Node 2: \" + node2 + \". Answer yes or no.\"}\n",
    "        ]\n",
    "    ) \n",
    "    return response.choices[0].message.content.strip().lower() == 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"MATCH (n) RETURN n\"\n",
    "result = graph.query(query)\n",
    "for record in result:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m relation\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m format_string(relation\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m relation\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m---> 16\u001b[0m graphStr \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_as_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(graphStr)\n",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m, in \u001b[0;36mget_graph_as_text\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     11\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m relation \u001b[38;5;129;01min\u001b[39;00m graph:\n\u001b[0;32m---> 13\u001b[0m     res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m format_string(relation\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m relation\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.query(query)\n",
    "\n",
    "def format_string(s: str) -> str:\n",
    "    return s.lower().replace('_', ' ')\n",
    "\n",
    "def get_graph_as_text(graph):\n",
    "    res = \"\"\n",
    "    for relation in graph:\n",
    "        res += '\"' + relation.get('r')[0].get('name') + '\" ' + format_string(relation.get('r')[1]) + ' \"' + relation.get('r')[2].get('name') + '\"' + \"\\n\"\n",
    "    return res\n",
    "\n",
    "graphStr = get_graph_as_text(result)\n",
    "print(graphStr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt_graph_consistent(graph: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[    \n",
    "            # System basically tells the chat gpt model how to act\n",
    "            {\"role\": \"system\", \"content\": \"You are the most intelligent consistency checker in the world. You can read any piece of text and answer whether it is logically consistent or not.\"},\n",
    "            {\"role\": \"user\", \"content\": 'Check if the following text is consistent. Be concise. Concepts and nouns are surrounded in double quotes, such as \"Event.\" The words in between two concepts are relationship identifiers that connect the two. If there are any logical errors, please list all logical errors. The text is: ' + \"\\n\" + graph}\n",
    "        ]\n",
    "    ) \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_gpt_graph_consistent(graphStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_nodes(graph):\n",
    "    query = \"MATCH (n) RETURN n\"\n",
    "    result = graph.query(query)\n",
    "    return result\n",
    "nodes = get_all_nodes(graph)\n",
    "# print(nodes[1])\n",
    "# compare each pair of nodes\n",
    "for i, node in enumerate(nodes):\n",
    "    for j, node2 in enumerate(nodes):\n",
    "        if i != j:\n",
    "            print(node)\n",
    "            if (ask_gpt_if_same(node.get('n').get('name'), node2.get('n').get('name'))):\n",
    "                print(f\"Node {i} and Node {j} mean the same thing.\")\n",
    "\n",
    "# for i, node in enumerate(nodes):\n",
    "#     print(i,node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_node_but_retain_edges(graph, node_id, new_node_id):\n",
    "    # Reassign relationships to the new node\n",
    "    query = f\"\"\"\n",
    "    MATCH (n {{id: '{node_id}'}})-[r]->(m)\n",
    "    CREATE (new {{id: '{new_node_id}'}})-[new_r:TYPE(r)]->(m)\n",
    "    SET new_r = r\n",
    "    WITH n, r\n",
    "    MATCH (m)-[r2]->(n)\n",
    "    CREATE (m)-[new_r2:TYPE(r2)]->(new {{id: '{new_node_id}'}})\n",
    "    SET new_r2 = r2\n",
    "    DELETE r, r2\n",
    "    \"\"\"\n",
    "    graph.query(query)\n",
    "    \n",
    "    # Delete the original node\n",
    "    delete_query = f\"MATCH (n {{id: '{node_id}'}}) DELETE n\"\n",
    "    graph.query(delete_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
